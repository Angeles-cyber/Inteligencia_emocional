from flask import Flask, request, jsonify, send_from_directory
from dotenv import load_dotenv
import os
import google.generativeai as genai

# ğŸ’¡ Cargamos las variables de entorno
load_dotenv()
api_key = os.getenv("GOOGLE_API_KEY")
print("ğŸ” API KEY cargada:", api_key)

# ğŸ¤– Configuramos el modelo de Gemini con nuestra API Key
genai.configure(api_key=api_key)
model = genai.GenerativeModel("models/gemini-1.5-flash")

# ğŸŒ Creamos la app de Flask
app = Flask(__name__)

# ğŸ§  Historial de la conversaciÃ³n (inicio con un mensaje guÃ­a)
chat_history = [
    {
        "role": "user",
        "parts": [
            "Eres un psicÃ³logo empÃ¡tico. Escucha con atenciÃ³n, responde con amabilidad, validando emociones y ofreciendo apoyo emocional, no consejos mÃ©dicos."
        ]
    }
]

@app.route("/")
def index():
    # ğŸ  Mostramos la interfaz principal (chat.html)
    return send_from_directory('.', 'chat.html')

@app.route("/chat", methods=["POST"])
def chat():
    global chat_history
    data = request.get_json()
    prompt = data.get("message", "")

    try:
        # ğŸ“© AÃ±adimos el mensaje del usuario al historial
        chat_history.append({"role": "user", "parts": [prompt]})

        # ğŸ§‘â€âš•ï¸ Iniciamos una sesiÃ³n de chat con historial
        chat_session = model.start_chat(history=chat_history)

        # âœ‰ï¸ Enviamos el mensaje al modelo
        response = chat_session.send_message(prompt)
        reply = response.text

        # ğŸ—£ï¸ Guardamos la respuesta del modelo
        chat_history.append({"role": "model", "parts": [reply]})

        # âœ… Devolvemos la respuesta al frontend
        return jsonify({"reply": reply})
    except Exception as e:
        import traceback
        traceback.print_exc()
        # âš ï¸ En caso de error, enviamos una respuesta amigable
        return jsonify({"reply": f"âŒ Ups... algo saliÃ³ mal: {str(e)}"}), 500

# ğŸš€ Ejecutamos la app en modo debug
if __name__ == "__main__":
    app.run(debug=True)
